@article{he2023gaia,
  title={GAIA: Zero-shot Talking Avatar Generation},
  author={He*, Tianyu and Guo*, Junliang and Yu*, Runyi and Wang*, Yuchi and Zhu, Jialiang and An, Kaikai and Li, Leyi and Tan, Xu and Wang, Chunyu and Hu, Han and others},
  journal={ICLR 2024},
  year={2023}
}

@article{chen2023towards,
  title={Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond},
  author={Chen, Liang and Zhang, Yichi and Ren, Shuhuai and Zhao, Haozhe and Cai, Zefan and Wang, Yuchi and Wang, Peiyi and Liu, Tianyu and Chang, Baobao},
  journal={FMDM@NeurIPS 2023},
  year={2023}
}

@article{bai2024uniedit,
  title={UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing},
  author={Bai, Jianhong and He, Tianyu and Wang, Yuchi and Guo, Junliang and Hu, Haoji and Liu, Zuozhu and Bian, Jiang},
  journal={arXiv preprint arXiv:2402.13185},
  year={2024}
}

@article{chen2024pca,
  title={PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain},
  author={Chen, Liang and Zhang, Yichi and Ren, Shuhuai and Zhao, Haozhe and Cai, Zefan and Wang, Yuchi and Wang, Peiyi and Meng, Xiangdi and Liu, Tianyu and Chang, Baobao},
  journal={arXiv preprint arXiv:2402.15527},
  year={2024}
}